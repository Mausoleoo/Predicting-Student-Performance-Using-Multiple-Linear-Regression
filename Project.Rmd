---
title: "Project Regression Models"
author: "Mauricio GÃ³mez Macedo"
date: "December 6, 2023"
output: html_document
---

# Table of content

- Introduction

- Dataset

  - Variables
  - Target Variable
  
- Exploratory Data Analysis
  - Histograms
  - Boxplots
  
- Multiple Linear Regression
  - Dummy variable
  - Correlation Map
  - Fitting
  - Stepwise process
  - Adequacy of the model
- Conclusion






# Introduction

In data science and statistical analysis, the application of regression models is a powerful tool for understanding and predicting relationships between variables. As part of our exploration into the diverse landscape of statistical methods, this project explores the complexities of implementing multiple linear regression. The focal point of our investigation is a comprehensive dataset capturing various facets of student performance.

The foundation of this project is grounded in the knowledge gained from our coursework, where we have assimilated the theoretical foundations and practical applications of multiple linear regression and statistical methods. As we traverse through this endeavour, we aim to apply these learned principles to unravel the underlying patterns within the dataset, shedding light on the factors influencing student performance.

The chosen dataset encompasses six variables, ranging from sleeping and studying to extracurricular activities. By employing multiple linear regression, we seek to construct a model that describes the relationships between these variables and enables us to make informed predictions about future student performance.

# Dataset

The Student Performance dataset comprises 10,000 observations, each encapsulating six features. This dataset promises a comprehensive overview of factors influencing academic performance. We aim to extract meaningful insights using statistical models, particularly multiple linear regression, to discern patterns within this extensive dataset.

### Variables

- Hours Studied: The total number of hours spent studying by each student.

- Previous Scores: The scores obtained by students in previous tests.

- Extracurricular Activities: Whether the student participates in extracurricular activities (Yes or No).

- Sleep Hours: The average number of hours of sleep the student had per day.

- Sample Question Papers Practiced: The number of sample question papers the student practiced.

### Target Variable

- Performance Index: A measure of the overall performance of each student. The performance index represents the student's academic performance and has been rounded to the nearest integer. The index ranges from 10 to 100, with higher values indicating better performance.

Source: https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression

```{r}
df <- read.csv("Student_Performance.csv")
head(df)
nrow(df)
colnames(df)
```

# Exploratory Data Analysis

### Histograms

```{r}
hist(df$Hours.Studied,main = "Hours Studied" , xlab = "Hours", ylab = "Frequency", col = "blue", border = "black")
```

```{r}
hist(df$Previous.Scores,main = "Previous Scores" , xlab = "Scores", ylab = "Frequency", col = "blue", border = "black")
```


```{r}
library(ggplot2)
ggplot(df, aes(x = Extracurricular.Activities, fill = Extracurricular.Activities)) +
  geom_bar() +
  labs(title = "Extracurricular Activities", x = "Response", y = "Count") +
  scale_fill_manual(values = c("No" = "red", "Yes" = "green"))
```
```{r}
hist(df$Sleep.Hours,main = "Sleep Hours" , xlab = "Hours", ylab = "Frequency", col = "blue", border = "black")
```

```{r}
hist(df$Sample.Question.Papers.Practiced,main = "Sample Question Papers Practiced" , xlab = "Sample Question Papers Practiced", ylab = "Frequency", col = "blue", border = "black")
```


```{r}
hist(df$Performance.Index,main = "Performance" , xlab = "Scores", ylab = "Frequency", col = "orange", border = "black")
```

In the histograms, it is evident that Hours.Studied, Previous.Scores, Extracurricular.Activities, Sleep.Hours, and Sample.Question.Papers.Practiced showcase a uniform distribution, implying an even distribution of values.

In contrast, the target variable Performance.Index deviates from this pattern, displaying characteristics consistent with a normal distribution. This deviation suggests a central tendency in Performance.Index, potentially following a bell-shaped curve. These distinct distributional features among predictors and the target variable offer valuable insights, forming a crucial foundation for our subsequent modelling and analytical endeavours.

```{r}
plot(df$Hours.Studied, df$Performance.Index,
     xlab = "Hours Studied",
     ylab = "Performance Index",
     col = "blue",
     pch = 16)

```

```{r}
ggplot(df, aes(x = Previous.Scores, y = Performance.Index)) +
  geom_point(color = "blue") 
```

It becomes apparent that the variables Previous.Scores and Performance.Index exhibit a discernible linear correlation. The plotted data points suggest a trend where an increase in Previous.Scores aligns with a corresponding rise in Performance.Index, indicative of a positive linear relationship between these two variables. This observation underscores the potential predictive power of Previous.Scores in influencing the overall performance index, providing a valuable insight for further analysis and model development.

### Boxplots


```{r}
boxplot(df[c(1,2,4,5,6)])
```


The boxplots for the features Hours.Studied, Previous.Scores, Sleep.Hours, Performance.Index, and Sample.Question.Papers.Practiced reveal a centralized distribution of data. This is evident as the medians are positioned near the center of each box, indicating a balanced distribution of values within these features. Notably, the absence of outliers in the boxplots suggests a consistent and concentrated spread of data points, reinforcing the stability of these variables.

# Multiple Linear Regression

### Dummy variable

As the the feature Extracurricular.Activities is a categorical variable with values of yes or no. It must be changed to a numerical value. That is why a dummy variable has been used. The value yes has been changed for 1 and no for 0.

```{r}

library(fastDummies)
library(dplyr)
df_dummy <- dummy_columns(df, select_columns = "Extracurricular.Activities")

df_total <- dplyr::select(df_dummy, -Extracurricular.Activities, -Extracurricular.Activities_No)
head(df_total)
```

### Correlation Map

```{r}
library(corrplot)

cor_matrix <- cor(df_total)

corrplot(cor_matrix, method = "color")
```


The correlation map highlights a strong association between the target feature and the variables Previous.Scores and Hours.Studied. These predictors exhibit a noteworthy correlation, emphasizing their potential impact on the target variable. This information guides our focus on key contributors as we proceed with the analysis.

### Fitting

```{r}
model <- lm(Performance.Index ~ ., data = df_total)
summary(model)
```

The P-values for each parameter in our analysis are remarkably low, approaching almost zero. This signifies that we cannot assert the value of any parameter is equal to zero. The low P-values indicate a high level of statistical significance, suggesting that each parameter plays a significant role in influencing the observed outcomes. 


The model achieved a remarkable R-squared of 0.9888, signifying that 98.88% of the dependent variable's variance is explained by the independent variables. The Adjusted R-squared, closely trailing at 0.9887, adjusts for predictor count, these high values suggest a strong fit.


```{r}
anova(model)
```

```{r}

sse <- sum(summary(model)$residuals^2)
ssr <- sum((predict(model) - mean(df_total$Performance.Index))^2)
sst <- sum((df_total$Performance.Index - mean(df_total$Performance.Index))^2)

# Display the results
print(paste("SSE:", sse))
print(paste("SSR:", ssr))
print(paste("SST:", sst))

```

A remarkably low Sum of Squares Error (SSE) suggests that the linear regression model effectively captures and explains the variability in the data, resulting in a highly precise fit.

```{r}
confint(model, level = 0.95)
```

These confidence intervals, calculated with a 95% confidence level, delineate the range of plausible values for each parameter in our model. They offer insight into the precision and reliability of our parameter estimates.

### Stepwise process

```{r}

step(model, direction = "both")
```


The stepwise process, guided by the Akaike Information Criterion (AIC), sequentially optimized the model by adding or removing variables. The decreasing AIC values (14246, 14468, 14961, 15727, 40733, 57567) indicate the iterative refinement, with the final model having the lowest AIC (14246), suggesting a well-balanced when all the predictors are used. 


### Adequacy of the model

```{r}
plot(model, which=1)
```

The residuals vs. fitted values plot shows a random and patternless distribution of dots with a uniform spread, indicating the model captures the relationship well and meets assumptions of linearity and homoscedasticity. This increases confidence in the model's reliability and predictive performance.


```{r}
plot(model, which=2)
```

The points in the QQ plot of residuals are close to the line, which indicates that the residuals follow a normal distribution. Having residuals that closely follow the line in a QQ plot is a good sign for the validity of the linear regression model. It implies that the model assumptions related to the distribution of errors are met, which enhances the reliability of statistical inferences and predictions made by the model. 



# Conclusion

In this project, we embarked on a comprehensive exploration of a dataset on student performance, applying statistical methods learned in the course, including multiple linear regression. The dataset, comprising 10,000 observations with six features, was a rich source for uncovering patterns influencing academic outcomes. We observed distributions and relationships through histograms, correlation maps, and boxplots, laying the foundation for our regression modelling.

The regression model, with an impressive R-squared of 0.9888 and an adjusted R-squared of 0.9887, showcased its effectiveness in explaining student performance. The low p-values of all parameters underscored their significance, and the confidence intervals at a 95% confidence level provided insights into parameter precision.

Further analysis involved examining residuals through QQ plots and residuals vs. fitted values plots. The QQ plot displayed adherence to normality assumptions, while the residuals vs. fitted values plot revealed a random, patternless scatter with a uniform spread, validating the model's linearity and homoscedasticity assumptions.

In conclusion, the project successfully applied regression techniques to understand and predict student performance with a robust model supported by thorough statistical diagnostics. The adherence to assumptions and the model's reliability encourage confidence in leveraging these insights for informed decision-making in educational contexts.


